# ğŸ¬ Synopsify - ì˜í™” ì¥ë¥´ ë¶„ë¥˜ í”„ë¡œì íŠ¸

ì˜í™” **ì¤„ê±°ë¦¬(plot)** ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ ì˜í™”ì˜ **ì¥ë¥´(genre)** ë¥¼ ì˜ˆì¸¡í•˜ëŠ”  
**ë¶„ë¥˜ ëª¨ë¸** ë¹„êµ í”„ë¡œì íŠ¸

- **í”„ë ˆì„ì›Œí¬**: PyTorch, Transformers
- **ëª¨ë¸**: LSTM, BERT-base
- **ì¥ë¥´ ìˆ˜**: 27ê°œ í´ë˜ìŠ¤

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” ì˜í™” ì¤„ê±°ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ 27ê°œ ì¥ë¥´ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” **Single-label ë¶„ë¥˜** ëª¨ë¸ì…ë‹ˆë‹¤.

- **ì…ë ¥**: ì˜í™” ì¤„ê±°ë¦¬ í…ìŠ¤íŠ¸ (ì˜ì–´)
- **ì¶œë ¥**: 27ê°œ ì¥ë¥´ ì¤‘ 1ê°œ ì˜ˆì¸¡
- **ì¥ë¥´ ì¢…ë¥˜**: action, comedy, drama, thriller, horror, sci-fi, romance ë“±


## ğŸ“Š ë°ì´í„°ì…‹

### ë°ì´í„° ì¶œì²˜
- **Kaggle**: [Genre Classification Dataset (IMDB)](https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb)

### ë°ì´í„° êµ¬ì„±
- **Train ë°ì´í„°**: 54,214ê°œ ìƒ˜í”Œ
- **Test ë°ì´í„°**: 54,200ê°œ ìƒ˜í”Œ
- **ì¥ë¥´ ìˆ˜**: 27ê°œ í´ë˜ìŠ¤
- **ë°ì´í„° í˜•ì‹**: `id ::: title ::: genre ::: plot`

### ë°ì´í„° íŠ¹ì§•
- ì¥ë¥´ ë¶ˆê· í˜• ì¡´ì¬ (drama, comedy ë“±ì´ ë§ìŒ)
- í…ìŠ¤íŠ¸ ê¸¸ì´ ë‹¤ì–‘ (ìµœëŒ€ 256 í† í°ìœ¼ë¡œ ì œí•œ)

### ë°ì´í„° ì „ì²˜ë¦¬
1. **í…ìŠ¤íŠ¸ ì •ì œ**: ì†Œë¬¸ì ë³€í™˜, êµ¬ë‘ì  ì œê±°, ê³µë°± ì •ë¦¬
2. **í† í°í™”**: ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
3. **Vocab ìƒì„±**: ìì£¼ ì“°ì¸ ìƒìœ„ 30,000ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš© (LSTM)
4. **ê¸¸ì´ í†µì¼**: ëª¨ë“  ë¬¸ì¥ì„ 256 í† í°ìœ¼ë¡œ ë§ì¶¤ (ë¶€ì¡±í•˜ë©´ padding, ë„˜ì¹˜ë©´ truncation)
5. **ë°ì´í„° ë¶„í• **: Train ë°ì´í„°ë¥¼ 8:2 ë¹„ìœ¨ë¡œ train/validation ë¶„í• 

## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

### 1. LSTM ëª¨ë¸

#### êµ¬ì¡°
```
Input â†’ Embedding â†’ Bidirectional LSTM â†’ Dropout â†’ FC Layer â†’ Output
```

#### ì£¼ìš” íŠ¹ì§•
- **Embedding**: ë‹¨ì–´ë¥¼ 128ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- **Bidirectional LSTM**: ì•â†’ë’¤, ë’¤â†’ì• ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ ì´í•´
  - NUM_LAYERS: 1
  - HIDDEN_DIM: 128
- **Dropout**: 0.4 (ê³¼ì í•© ë°©ì§€)
- **FC Layer**: 27ê°œ ì¥ë¥´ ë¶„ë¥˜

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
| í•­ëª© | LSTM | LSTM CW |
|------|-----------|----------|
| Vocab Size | 30,000 | 30,000 |
| Embedding Dim | 128 | 128 |
| Hidden Dim | 128 | 128 |
| NUM_LAYERS | 1 | 1 |
| Bidirectional | âœ… | âœ… |
| Dropout | 0.4 | 0.4 |
| Max Seq Length | 256 | 256 |
| Optimizer | Adam | Adam |
| Learning Rate | 5e-4 | 5e-4 |
| Batch Size | 64 | 64 |
| Epochs | 10 | 10 |
| Class Weight | âŒ | âœ… |

**LSTM CW**: Class Weightë¥¼ ì ìš©í•˜ì—¬ ë“œë¬¸ ì¥ë¥´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

---

### 2. BERT ëª¨ë¸

#### êµ¬ì¡°
```
Input â†’ Embedding (Token + Position) â†’ Transformer (12 layers) â†’ Classification Head â†’ Output
```

#### ì£¼ìš” íŠ¹ì§•
- **Pretrained BERT-base**: ì´ë¯¸ í•™ìŠµëœ ì–¸ì–´ ì´í•´ ëª¨ë¸ ì‚¬ìš©
  - NUM_LAYERS: 12
  - HIDDEN_DIM: 768
- **Classification Head**: 27ê°œ ì¥ë¥´ ë¶„ë¥˜ìš© ë ˆì´ì–´ ì¶”ê°€
- **Fine-tuning**: ì˜í™” ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ

#### í•˜ì´í¼íŒŒë¼ë¯¸í„°
| í•­ëª© | ê°’ |
|------|-----|
| Model | bert-base-uncased |
| Transformer Layers | 12 (ì‚¬ì „ ì„¤ì •) |
| Hidden Size | 768 (ì‚¬ì „ ì„¤ì •) |
| Max Length | 256 |
| Optimizer | AdamW |
| Learning Rate | 2e-5 |
| Batch Size | 16 |
| Epochs | 3 |
| Warmup Ratio | 0.1 |

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼

### ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | Accuracy | F1-macro | F1-micro | Loss |
|------|----------|----------|----------|------|
| **LSTM** | 0.5376 | 0.1847 | 0.5376 | 1.6593 |
| **LSTM CW** | 0.4856 | 0.2646 | 0.4856 | 2.2267 |
| **BERT** | **0.6947** | **0.4838** | **0.6947** | **1.0699** |

### ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **BERTê°€ ëª¨ë“  ì§€í‘œì—ì„œ ìµœê³  ì„±ëŠ¥**
   - Accuracy: +15.7%p (vs LSTM High)
   - F1-macro: +29.9%p (vs LSTM High)

2. **LSTM CWì˜ Class Weight íš¨ê³¼**
   - F1-macroëŠ” í–¥ìƒ (0.1847 â†’ 0.2646)
   - AccuracyëŠ” í•˜ë½ (0.5376 â†’ 0.4856)

3. **Pretrained ëª¨ë¸ì˜ íš¨ê³¼ í™•ì¸**
   - BERTì˜ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì´ ì¥ë¥´ ë¶„ë¥˜ì— ìœ ë¦¬
   - ì ì€ Epochs(3)ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ë°ì´í„° ì¤€ë¹„

1. Kaggleì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
   - [Genre Classification Dataset (IMDB)](https://www.kaggle.com/datasets/hijest/genre-classification-dataset-imdb)

2. `data/` í´ë”ì— íŒŒì¼ ë³µì‚¬
   ```
   data/
   â”œâ”€â”€ train_data.txt
   â”œâ”€â”€ test_data.txt
   â””â”€â”€ test_data_solution.txt
   ```

### 2. ëª¨ë¸ í•™ìŠµ

#### LSTM ëª¨ë¸ í•™ìŠµ
```bash
# LSTM (Class Weight ì—†ìŒ)
jupyter notebook lstm_train.ipynb

# LSTM CW(Class Weight ì ìš©)
jupyter notebook lstm_CW_train.ipynb
```

#### BERT ëª¨ë¸ í•™ìŠµ
```bash
jupyter notebook bert_train.ipynb
```

### 3. ì˜ˆì¸¡

í•™ìŠµëœ ëª¨ë¸ë¡œ ì¥ë¥´ ì˜ˆì¸¡:
```bash
jupyter notebook predict.ipynb
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# LSTM ëª¨ë¸ ì‚¬ìš©
ì˜ˆì¸¡ ì¥ë¥´: comedy (confidence=0.118)

# BERT ëª¨ë¸ ì‚¬ìš©
ì˜ˆì¸¡ ì¥ë¥´: fantasy (confidence=0.331)
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
Synopsify/
â”œâ”€â”€ data/                      # ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ train_data.txt
â”‚   â”œâ”€â”€ test_data.txt
â”‚   â””â”€â”€ test_data_solution.txt
â”œâ”€â”€ model/                     # í•™ìŠµëœ ëª¨ë¸
â”‚   â”œâ”€â”€ lstm.pt
â”‚   â”œâ”€â”€ lstm_CW.pt
â”‚   â””â”€â”€ bert/
â”œâ”€â”€ lstm_train.ipynb          # LSTM í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ lstm_CW_train.ipynb       # LSTM CW í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ bert_train.ipynb          # BERT í•™ìŠµ ì½”ë“œ
â”œâ”€â”€ predict.ipynb             # ì˜ˆì¸¡ ì½”ë“œ
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```


## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
```
python
torch
transformers
pandas
numpy
scikit-learn
tqdm
```

### ì„¤ì¹˜ ë°©ë²•
```bash
pip install torch transformers pandas numpy scikit-learn tqdm
```

## ğŸ“ ì£¼ìš” ì‹¤í—˜ ë³€ê²½ì‚¬í•­

1. **Loss Function**: CrossEntropyLoss â†’ Weighted CrossEntropyLoss (LSTM CW)
2. **Pretrained Model**: None â†’ BERT-base (BERT)
3. **Optimizer**: Adam â†’ AdamW (BERT)
4. **Scheduler**: None â†’ Linear Warmup (BERT)

## ğŸ¯ ê²°ë¡ 

### ì£¼ìš” ì„±ê³¼
- âœ… **BERT-base ëª¨ë¸ì´ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±** (Accuracy: 69.47%)
- âœ… **Pretrained ëª¨ë¸ì˜ íš¨ê³¼ í™•ì¸**
- âœ… **Class Weight ì ìš©ìœ¼ë¡œ F1-macro ê°œì„ ** 

### í•œê³„ì  ë° ê°œì„  ë°©í–¥
- F1-macroê°€ ì—¬ì „íˆ ë‚®ìŒ (0.48) â†’ ì¥ë¥´ ë¶ˆê· í˜• ë¬¸ì œ
- 27ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ëŠ” ì–´ë ¤ìš´ íƒœìŠ¤í¬
- **í–¥í›„ ê°œì„ **: ë” í° BERT ëª¨ë¸, Data Augmentation, Ensemble